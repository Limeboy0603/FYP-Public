capture:
  # METHOD 1:
  # Set source to the ID of the camera
  # Windows: Get-CimInstance Win32_PnPEntity | ? { $_.service -eq "usbvideo" } | Select-Object -Property PNPDeviceID, Name
  # Linux: ls /dev/video*
  # MacOS: system_profiler SPCameraDataType
  source: 0

  # METHOD 2:
  # Set the path of a testing video
  # source: dataset/full_test/final.avi

  # Set the resolution of the capture
  # Format: {width}x{height}
  resolution: 1920x1080

paths:
  # Set model path
  model: models/final_full.keras

  # Set keypoint saved path. Should be a directory.
  keypoints: dataset/keypoints

  # Set model checkpoint path. Should be a file.
  model_checkpoint: model_checkpoint/model_checkpoint.keras

  # Set the path to dataset. 
  # In the dataset, each directory should be a class
  # Each class contains image samples
  dataset: dataset/clips

  # Set the path to keypoints after perfornming train_test_split
  # Directory structure:
  # split
  # |--test
  # |--train
  # |--val
  split: dataset/split

  # Set the directory name to store the language model and it's training result
  llm: llm


# The following settings are only for capturing via webcam

# Define the class samples to record using cam_capture.py
dictionary:
  # The glosses for self introduction
  - "#BLANK"
  - Hello
  - Me
  - English
  - Name

  # Letters from the alphabet
  - A
  - B
  - C
  - D
  - E
  - F
  - G
  - H
  - I
  - J
  - K
  - L
  - M
  - N
  - O
  - P
  - Q
  - R
  - S
  - T
  - U
  - V
  - W
  - X
  - Y
  - Z

sequence:
  # Set the number of frames to capture for each gesture
  frame: 30

  # Set the number of samples for each gesture
  count: 100


# The following settings are only for training the LLM
llm_samples:
  - hksl: Hello Me English Name A B C D
    natural: Hello, my English name is ABCD.