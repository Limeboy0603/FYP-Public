{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 64\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = \"torch\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "# TEST: force CPU\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from config import config_parser\n",
    "from mp_util_legacy import preprocess_keypoints_multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, keypoint_path, seq_max_len, batch_size=32, shuffle=False, transform=False):\n",
    "        self.keypoint_path = keypoint_path\n",
    "        self.seq_max_len = seq_max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.file_list = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.all_labels = sorted(os.listdir(keypoint_path))\n",
    "        for label in self.all_labels:\n",
    "            for file in os.listdir(os.path.join(keypoint_path, label)):\n",
    "                self.file_list.append(file)\n",
    "                self.labels.append(label)\n",
    "        self.epoch_count = -1\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def reset_epoch(self):\n",
    "        self.epoch_count = -1\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.epoch_count += 1\n",
    "        np.random.seed(self.epoch_count * 42)\n",
    "        self.indexes = np.arange(len(self.file_list))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.file_list) / self.batch_size))\n",
    "    \n",
    "    def preprocess_keypoints(self, keypoints):\n",
    "        angle = np.random.randint(-20, 20)\n",
    "        tx = np.random.uniform(-0.4, 0.4)\n",
    "        ty = np.random.uniform(-0.4, 0.4)\n",
    "        scale = np.random.uniform(0.6, 1.2)\n",
    "        return preprocess_keypoints_multiple(keypoints, angle=angle, tx=tx, ty=ty, scale=scale)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        sequences = []\n",
    "        labels = []\n",
    "\n",
    "        for index in indexes:\n",
    "            kp_file_name = self.file_list[index]\n",
    "            label = self.labels[index]\n",
    "            sequences.append(np.load(os.path.join(self.keypoint_path, label, kp_file_name), mmap_mode=\"r\"))\n",
    "            labels.append(label)\n",
    "\n",
    "        for i in range(len(sequences)):\n",
    "            if self.transform:\n",
    "                sequences[i] = self.preprocess_keypoints(sequences[i])\n",
    "            sequences[i] = sequences[i].reshape(self.seq_max_len, -1)\n",
    "\n",
    "        X = np.array(sequences)\n",
    "        Y = np.array([self.all_labels.index(label) for label in labels])\n",
    "        return X, Y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X, Y = self.__data_generation(indexes)\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = KeypointDataGenerator(\n",
    "    \"../dataset/split/train\", \n",
    "    seq_max_len=30, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    transform=True\n",
    ")\n",
    "validation_generator = KeypointDataGenerator(\n",
    "    \"../dataset/split/val\",\n",
    "    seq_max_len=30, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    transform=False\n",
    ")\n",
    "test_generator = KeypointDataGenerator(\n",
    "    \"../dataset/split/test\",\n",
    "    seq_max_len=30, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    transform=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, learning_rate, epochs):\n",
    "    start_time = time.time()\n",
    "    training_generator.reset_epoch()\n",
    "    validation_generator.reset_epoch()\n",
    "    test_generator.reset_epoch()\n",
    "    model.summary()\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # print(\"Device: {}\".format(device))\n",
    "    # model.to(device)\n",
    "    history = model.fit(\n",
    "        training_generator,\n",
    "        validation_data=validation_generator,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "            # keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "        ]\n",
    "    )\n",
    "    # plot training accuracy and validation accuracy in the same plot\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "    # plot training loss and validation loss in the same plot\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(test_generator)\n",
    "    print('Test loss: {}'.format(test_loss))\n",
    "    print('Test accuracy: {}'.format(test_acc))\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time\n",
    "\n",
    "    # return training accuracy of last epoch, validation accuracy of last epoch, test accuracy\n",
    "    return history.history['accuracy'][-1], history.history['val_accuracy'][-1], test_acc, time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = training_generator.__getitem__(0)\n",
    "X_shape = first_batch[0].shape[1:]\n",
    "Y_shape = len(training_generator.all_labels)\n",
    "del first_batch\n",
    "print(X_shape, Y_shape)\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1: LSTM vs GRU (Single Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test1 = Sequential([\n",
    "    LSTM(128, input_shape=X_shape, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "results[\"LSTM_single_layer\"] = train_model(model_test1, 0.0001, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test2 = Sequential([\n",
    "    GRU(128, input_shape=X_shape, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "results[\"GRU_single_layer\"] = train_model(model_test2, 0.0001, EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: LSTM vs GRU (Multi Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test3 = Sequential([\n",
    "    LSTM(128, input_shape=X_shape, return_sequences=True),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "results[\"LSTM_double_layer\"] = train_model(model_test3, 0.0001, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test4 = Sequential([\n",
    "    GRU(128, input_shape=X_shape, return_sequences=True),\n",
    "    GRU(128, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "results[\"GRU_double_layer\"] = train_model(model_test4, 0.0001, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in results.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know LSTM singlelayer is better, we test different learning rates.\n",
    "\n",
    "# Test 3: Learning rates\n",
    "\n",
    "Note: `lr=0.0001`, Test accuracy: 0.91796875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: labtop broke at this moment, manually re-creating the results\n",
    "\"\"\"\n",
    "LSTM_single_layer (0.8634868264198303, 0.921875, 0.91796875, 1516.46448636055)\n",
    "GRU_single_layer (0.6940789222717285, 0.765625, 0.75390625, 1714.1956386566162)\n",
    "LSTM_double_layer (0.7985197305679321, 0.84765625, 0.87890625, 1329.3484818935394)\n",
    "GRU_double_layer (0.7080591917037964, 0.69921875, 0.734375, 1714.6088211536407)\n",
    "\"\"\"\n",
    "results = {\n",
    "    \"LSTM_single_layer\": (0.8634868264198303, 0.921875, 0.91796875, 1516.46448636055),\n",
    "    \"GRU_single_layer\": (0.6940789222717285, 0.765625, 0.75390625, 1714.1956386566162),\n",
    "    \"LSTM_double_layer\": (0.7985197305679321, 0.84765625, 0.87890625, 1329.3484818935394),\n",
    "    \"GRU_double_layer\": (0.7080591917037964, 0.69921875, 0.734375, 1714.6088211536407),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test5 = Sequential([\n",
    "    LSTM(128, input_shape=X_shape, return_sequences=False),\n",
    "    # LSTM(128, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "results[\"LSTM_single_layer_lr_0.00001\"] = train_model(model_test5, 0.00001, EPOCH)\n",
    "print(\"LSTM_single_layer_lr_0.00001\", results[\"LSTM_single_layer_lr_0.00001\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test6 = Sequential([\n",
    "    LSTM(128, input_shape=X_shape, return_sequences=False),\n",
    "    # LSTM(128, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "results[\"LSTM_single_layer_lr_0.001\"] = train_model(model_test6, 0.001, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test7 = Sequential([\n",
    "    LSTM(128, input_shape=X_shape, return_sequences=False),\n",
    "    # LSTM(128, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "results[\"LSTM_single_layer_lr_0.01\"] = train_model(model_test7, 0.01, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test8 = Sequential([\n",
    "    LSTM(128, input_shape=X_shape, return_sequences=False),\n",
    "    # LSTM(128, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "results[\"LSTM_single_layer_lr_0.1\"] = train_model(model_test8, 0.1, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in results.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointing\n",
    "\"\"\"\n",
    "LSTM_single_layer (0.8634868264198303, 0.921875, 0.91796875, 1516.46448636055)\n",
    "GRU_single_layer (0.6940789222717285, 0.765625, 0.75390625, 1714.1956386566162)\n",
    "LSTM_double_layer (0.7985197305679321, 0.84765625, 0.87890625, 1329.3484818935394)\n",
    "GRU_double_layer (0.7080591917037964, 0.69921875, 0.734375, 1714.6088211536407)\n",
    "LSTM_single_layer_lr_0.00001 (0.45106908679008484, 0.5078125, 0.5, 958.5009093284607)\n",
    "LSTM_single_layer_lr_0.001 (0.7569901347160339, 0.796875, 0.86328125, 772.0105435848236)\n",
    "LSTM_single_layer_lr_0.01 (0.7405427694320679, 0.8515625, 0.8125, 967.8330719470978)\n",
    "LSTM_single_layer_lr_0.1 (0.18832236528396606, 0.23828125, 0.2265625, 637.1956248283386)\n",
    "\"\"\"\n",
    "\n",
    "results = {\n",
    "    \"LSTM_single_layer\": (0.8634868264198303, 0.921875, 0.91796875, 1516.46448636055),\n",
    "    \"GRU_single_layer\": (0.6940789222717285, 0.765625, 0.75390625, 1714.1956386566162),\n",
    "    \"LSTM_double_layer\": (0.7985197305679321, 0.84765625, 0.87890625, 1329.3484818935394),\n",
    "    \"GRU_double_layer\": (0.7080591917037964, 0.69921875, 0.734375, 1714.6088211536407),\n",
    "    \"LSTM_single_layer_lr_0.00001\": (0.45106908679008484, 0.5078125, 0.5, 958.5009093284607),\n",
    "    \"LSTM_single_layer_lr_0.001\": (0.7569901347160339, 0.796875, 0.86328125, 772.0105435848236),\n",
    "    \"LSTM_single_layer_lr_0.01\": (0.7405427694320679, 0.8515625, 0.8125, 967.8330719470978),\n",
    "    \"LSTM_single_layer_lr_0.1\": (0.18832236528396606, 0.23828125, 0.2265625, 637.1956248283386),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we choose `lr=0.0001`\n",
    "\n",
    "# Test 4: LSTM Unit count\n",
    "\n",
    "Note: 128 units, lr=`0.0001`, Test accuracy: 0.91796875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test9 = Sequential([\n",
    "    LSTM(64, input_shape=X_shape, return_sequences=False),\n",
    "    # LSTM(64, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "results[\"LSTM_single_layer_64\"] = train_model(model_test9, 0.0001, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test10 = Sequential([\n",
    "    LSTM(256, input_shape=X_shape, return_sequences=False),\n",
    "    # LSTM(256, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "results[\"LSTM_single_layer_256\"] = train_model(model_test10, 0.0001, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test11 = Sequential([\n",
    "    LSTM(512, input_shape=X_shape, return_sequences=False),\n",
    "    # LSTM(512, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "results[\"LSTM_single_layer_512\"] = train_model(model_test11, 0.0001, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test12 = Sequential([\n",
    "    LSTM(1024, input_shape=X_shape, return_sequences=False),\n",
    "    # LSTM(1024, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "results[\"LSTM_single_layer_1024\"] = train_model(model_test12, 0.0001, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test13 = Sequential([\n",
    "    LSTM(2048, input_shape=X_shape, return_sequences=False),\n",
    "    # LSTM(2048, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "results[\"LSTM_single_layer_2048\"] = train_model(model_test13, 0.0001, EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in results.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "\n",
    "\"\"\"\n",
    "LSTM_single_layer (0.8634868264198303, 0.921875, 0.91796875, 1516.46448636055)\n",
    "GRU_single_layer (0.6940789222717285, 0.765625, 0.75390625, 1714.1956386566162)\n",
    "LSTM_double_layer (0.7985197305679321, 0.84765625, 0.87890625, 1329.3484818935394)\n",
    "GRU_double_layer (0.7080591917037964, 0.69921875, 0.734375, 1714.6088211536407)\n",
    "LSTM_single_layer_lr_0.00001 (0.45106908679008484, 0.5078125, 0.5, 958.5009093284607)\n",
    "LSTM_single_layer_lr_0.001 (0.7569901347160339, 0.796875, 0.86328125, 772.0105435848236)\n",
    "LSTM_single_layer_lr_0.01 (0.7405427694320679, 0.8515625, 0.8125, 967.8330719470978)\n",
    "LSTM_single_layer_lr_0.1 (0.18832236528396606, 0.23828125, 0.2265625, 637.1956248283386)\n",
    "LSTM_single_layer_64 (0.7582237124443054, 0.7734375, 0.7734375, 980.6074757575989)\n",
    "LSTM_single_layer_256 (0.8955591917037964, 0.8984375, 0.92578125, 930.5116012096405)\n",
    "LSTM_single_layer_512 (0.8289473652839661, 0.859375, 0.8828125, 546.1381299495697)\n",
    "LSTM_single_layer_1024 (0.7615131735801697, 0.7578125, 0.83984375, 408.09040093421936)\n",
    "LSTM_single_layer_2048 (0.8252466917037964, 0.86328125, 0.8984375, 540.9860851764679)\n",
    "\"\"\"\n",
    "\n",
    "results = {\n",
    "    \"LSTM_single_layer\": (0.8634868264198303, 0.921875, 0.91796875, 1516.46448636055),\n",
    "    \"GRU_single_layer\": (0.6940789222717285, 0.765625, 0.75390625, 1714.1956386566162),\n",
    "    \"LSTM_double_layer\": (0.7985197305679321, 0.84765625, 0.87890625, 1329.3484818935394),\n",
    "    \"GRU_double_layer\": (0.7080591917037964, 0.69921875, 0.734375, 1714.6088211536407),\n",
    "    \"LSTM_single_layer_lr_0.00001\": (0.45106908679008484, 0.5078125, 0.5, 958.5009093284607),\n",
    "    \"LSTM_single_layer_lr_0.001\": (0.7569901347160339, 0.796875, 0.86328125, 772.0105435848236),\n",
    "    \"LSTM_single_layer_lr_0.01\": (0.7405427694320679, 0.8515625, 0.8125, 967.8330719470978),\n",
    "    \"LSTM_single_layer_lr_0.1\": (0.18832236528396606, 0.23828125, 0.2265625, 637.1956248283386),\n",
    "    \"LSTM_single_layer_64\": (0.7582237124443054, 0.7734375, 0.7734375, 980.6074757575989),\n",
    "    \"LSTM_single_layer_256\": (0.8955591917037964, 0.8984375, 0.92578125, 930.5116012096405),\n",
    "    \"LSTM_single_layer_512\": (0.8289473652839661, 0.859375, 0.8828125, 546.1381299495697),\n",
    "    \"LSTM_single_layer_1024\": (0.7615131735801697, 0.7578125, 0.83984375, 408.09040093421936),\n",
    "    \"LSTM_single_layer_2048\": (0.8252466917037964, 0.86328125, 0.8984375, 540.9860851764679),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that single layer indeed outperform multi-layer regardless of units\n",
    "model_verify = Sequential([\n",
    "    LSTM(256, input_shape=X_shape, return_sequences=True),\n",
    "    LSTM(256, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "print(train_model(model_verify, 0.0001, EPOCH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that learning rate 0.0001 is correct by training 0.00001 and 0.001\n",
    "model_verify2 = Sequential([\n",
    "    LSTM(256, input_shape=X_shape, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "print(train_model(model_verify2, 0.001, EPOCH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_verify3 = Sequential([\n",
    "    LSTM(256, input_shape=X_shape, return_sequences=False),\n",
    "    Dense(Y_shape, activation='softmax')\n",
    "])\n",
    "print(train_model(model_verify3, 0.00001, EPOCH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "From the above experiments, we get\n",
    "- LSTM is **way** better than GRU\n",
    "- Single layer is better\n",
    "- `lr=0.0001`\n",
    "- Units: `256`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
